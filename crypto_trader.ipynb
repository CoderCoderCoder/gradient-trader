{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cryptocurrency Trader Agent\n",
    "#### Deep Q-Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cryptocurrency Market Simulator\n",
    "\n",
    "class Crypto_Simulator:\n",
    "    \n",
    "    states = ['bollinger', 'rolling20dayMeanReturn', 'volatility']\n",
    "    actions = ['buy', 'sell', 'hold']\n",
    "    \n",
    "    def init(num_steps):\n",
    "        return\n",
    "        \n",
    "    def get_ran_action():\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "    def step(action):\n",
    "        state = ''\n",
    "        reward = 0.0\n",
    "        return [state, reward]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Value Function Approximator\n",
    "# Neural Network Implementation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "class QValue_NN:\n",
    "    \n",
    "    _alpha = None\n",
    "    _layer = None\n",
    "    \n",
    "    # define input and target shapes\n",
    "    _state_size = None\n",
    "    _action_size = None\n",
    "    \n",
    "    _model = None\n",
    "\n",
    "    \n",
    "    # init neural network\n",
    "    def __init__(self, state_size, action_size, learning_rate, units):\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size\n",
    "        self._alpha = learning_rate\n",
    "        self._units = units\n",
    "        self._model = self._build_model()\n",
    "    \n",
    "    \n",
    "    # define loss function\n",
    "    def _huber_loss(self, target, prediction):\n",
    "        # sqrt(1+error^2)-1\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
    "\n",
    "    \n",
    "    # neural net for Deep-Q Learning Model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self._units, input_dim=self._state_size, activation='relu'))\n",
    "        model.add(Dense(self._units, activation='relu'))\n",
    "        model.add(Dense(self._action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self._alpha))\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # online training\n",
    "    def train(self, states, qvalues):\n",
    "        _model.fit(state, qvalues, epochs=1, verbose=0)\n",
    "    \n",
    "    \n",
    "    # get q-values based on states\n",
    "    def predict(self, states):\n",
    "        qvalues = _model.predict(state)\n",
    "        return qvalues\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 24)                2424      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 3,124\n",
      "Trainable params: 3,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a = QValue_NN(100, 4, 0.0005, 24)\n",
    "a._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cryptocurrency Trader Q-Learning Implementation\n",
    "\n",
    "states = ['bollinger', 'rolling20dayMeanReturn', 'volatility']\n",
    "actions = ['buy', 'sell', 'hold']\n",
    "\n",
    "class Crypto_Trader:\n",
    "\n",
    "    def init():\n",
    "        # Initialize RL parameters\n",
    "\n",
    "        # Learning Rate\n",
    "        alpha = 0.1 \n",
    "\n",
    "        # Reward Discount Rate\n",
    "        y = 0.95\n",
    "\n",
    "        # Esiplon (exploration factor)\n",
    "        esiplon = 0.1\n",
    "\n",
    "        # Reduce exploration overtime\n",
    "        esiplon_decay = 0.90\n",
    "\n",
    "        # number of episodes for training\n",
    "        num_episodes = 1000\n",
    "\n",
    "    def train():\n",
    "        \n",
    "        sim = Crypto_Simulator()\n",
    "        nn = QValue_NN(len(states), len(actions), alpha, 24)\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            #init simulator and get first new observation\n",
    "            s = sim.init()\n",
    "            ret = 0\n",
    "            iteration = 0\n",
    "            \n",
    "            #decay Esiplon\n",
    "            esiplon = esiplon * (esiplon_decay ** iteration)\n",
    "            \n",
    "            #Iterate through all experiences and update Q function approximator\n",
    "            while (1==1):\n",
    "                \n",
    "                iteration += 1\n",
    "                \n",
    "                #Choose action by e-greedy\n",
    "                if np.random.rand(1) < esiplon:\n",
    "                    a = sim.get_ran_action()\n",
    "                else:\n",
    "                    #Get Q values, choose action by Q values\n",
    "                    Qvalues = nn.predict(s)\n",
    "                    a = np.argmax(Qvalues)\n",
    "                    \n",
    "                #step to the next state and reward based on action\n",
    "                s1,r = sim.step(a)\n",
    "                \n",
    "                if (s1 == ''):\n",
    "                    break\n",
    "                \n",
    "                #Get Q values for s1\n",
    "                Qvalues1 = nn.predict(s1)\n",
    "                maxQ1 = np.max(Qvalues1)\n",
    "                \n",
    "                #Update TargetQ\n",
    "                targetQ = Qvalues\n",
    "                targetQ[a] = r + y*maxQ1\n",
    "                \n",
    "                #Train NN model online\n",
    "                nn.train(s, targetQ)\n",
    "                \n",
    "                #Update total reward, set s1 to s\n",
    "                ret += r\n",
    "                s = s1\n",
    "                \n",
    "            print ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
